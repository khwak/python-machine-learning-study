{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe46e061-3339-4e0c-a867-5e443f512a61",
   "metadata": {},
   "source": [
    "# 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0c255-ee36-4ba9-b43b-e342cd2f1858",
   "metadata": {},
   "source": [
    "+ 텍스트 분류 text classification\n",
    "+ 감성 분석 sentiment analysis\n",
    "+ 텍스트 요약 summarization\n",
    "+ 텍스트 군집화 clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73b9d8-8afb-4a79-968c-bc74b8672963",
   "metadata": {},
   "source": [
    "## 텍스트 분석 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699cb7d-35bb-4786-b178-897af92d9c06",
   "metadata": {},
   "source": [
    "텍스트 분석은 비정형 데이터인 텍스트를 분석하는 것이다.\n",
    "\n",
    "텍스트를 머신러닝에 적용하기 위해서는 비정형 텍스트 데이터를 어떻게 피처 형태로 추출하고 추출된 피처에 의미 있는 값을 부여하는가 하는 것이 매우 중요한 요소이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a1b8c-2e8f-4487-a3b0-ef1183a94567",
   "metadata": {},
   "source": [
    "피처 벡터화 feature vectorization 또는 피처 추출 feature extraction : 텍스트를 word 기반의 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하면 텍스트는 단어의 조합인 벡터값으로 표현될 수 있음.\n",
    "\n",
    "+ BOW bag of words\n",
    "+ Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8cc0c-e835-4d8f-b21c-276e7ef23409",
   "metadata": {},
   "source": [
    "### 텍스트 분석 수행 프로세스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4aa155-c951-4817-9d9d-2958920ce1b3",
   "metadata": {},
   "source": [
    "+ 텍스트 사전 준비작업(전처리)\n",
    "+ 피처 벡터화/추출\n",
    "+ ML 모델 수립 및 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6a5ad-c1ff-4c1c-8d9d-3876d1baf00a",
   "metadata": {},
   "source": [
    "### 파이썬 기반의 NLP, 텍스트 분석 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc6b8d-5697-49ca-8f30-c2d49ab91fdc",
   "metadata": {},
   "source": [
    "+ NLTK natural language toolkit for python\n",
    "+ Gensim\n",
    "+ SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0777520-1d5d-4242-9fe7-e568e401164d",
   "metadata": {},
   "source": [
    "## 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad49a1-2de9-4f0c-9bd2-671152655036",
   "metadata": {},
   "source": [
    "텍스트를 머신러닝 알고리즘이나 NLP 애플리케이션에입력 데이터로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 다양한 텍스트 데이터의 사전 작업을 수행하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955698fc-d453-4353-830c-06866719684d",
   "metadata": {},
   "source": [
    "+ 클렌징 cleansing\n",
    "+ 토큰화 tokenization\n",
    "+ 필터링/스톱 워드 제거/철자 수정\n",
    "+ Stemming\n",
    "+ Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec51e42-2d34-4b40-8c08-a6f0f78117eb",
   "metadata": {},
   "source": [
    "### 클렌징\n",
    "텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea00f7d-ffc5-4800-84df-e289d95bdb3d",
   "metadata": {},
   "source": [
    "### 텍스트 토큰화\n",
    "+ 문장 토큰화\n",
    "+ 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48273de4-119f-4296-9459-e1d9aef064c1",
   "metadata": {},
   "source": [
    "#### 문장 토큰화 sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2363600-8438-417a-b65a-e5b2b8d88d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SM-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe5027c-e601-4892-b18f-a44533ac7200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\SM-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecdf67e7-520f-438b-8d29-b5cfe6dcba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26865163-7d20-46ce-8a49-2c304975b87d",
   "metadata": {},
   "source": [
    "#### 단어 토큰화 word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040657f5-0ca8-4b5a-900d-caa3a08de008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018d285-f183-47a8-9671-bfb6e178d047",
   "metadata": {},
   "source": [
    "#### 문장 별 단어 토큰화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703d87c2-8f20-4b98-802b-8dbc33aedbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6368b-22ed-4d84-a628-8bf9d40f291f",
   "metadata": {},
   "source": [
    "문장을 단어별로 하나씩 토큰화할 경우 문맥적인 의미가 무시되기 때문에 이를 해결해 보고자 n-gram 도입\n",
    "+ n-gram : 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4da4b8-1033-40d7-b640-783cd80ba7f9",
   "metadata": {},
   "source": [
    "### 스톱 워드 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d9e75-fdc0-4d56-bea8-a4b7c5114b53",
   "metadata": {},
   "source": [
    "스톱 워드 stop word는 분석에 큰 의미가 없는 단어를 지칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54a9093-2c7e-4ded-bb55-847599d4542e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SM-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4012bbf3-9e9c-4fa4-b737-6e4e274f90d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb63fb4-03fc-4fb7-8330-7d0adc3340d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e3ef4-6f51-4f6c-ba8c-ffc300a5b5a9",
   "metadata": {},
   "source": [
    "is, this와 같은 스톱 워드가 제거됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7d8b1-5038-494f-a3f3-ecc29c1e2fb7",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81be0f-968f-43ed-a44b-b9063207fcc7",
   "metadata": {},
   "source": [
    "문법적 또는 의미적으로 변화하는 단어의 원형을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf519f6d-44f8-47b3-a9f8-115ceaf6f624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7277be-59b2-4eee-b0d1-d2e04706d472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\SM-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4ce13-f673-49a7-89f6-e363f9637c95",
   "metadata": {},
   "source": [
    "## Bag of Words - BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d77580-dd21-4a5b-b965-a4757a58595a",
   "metadata": {},
   "source": [
    "문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a93d2d-7892-4c13-a230-3e07acadb413",
   "metadata": {},
   "source": [
    "장점\n",
    "+ 쉽고 빠른 구축\n",
    "\n",
    "단점\n",
    "+ 문맥 의미 반영 부족\n",
    "+ 희소 행렬 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b2a75-f629-476e-91fa-188f2558fb65",
   "metadata": {},
   "source": [
    "### BOW 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04951b2d-a68f-4516-89c1-65dd5a8fda41",
   "metadata": {},
   "source": [
    "+ 카운트 기반의 벡터화\n",
    "+ TF-IDF(Term Frequency - Inverse Document Frequency) 기반의 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8867721-94f2-477a-b7fd-b08ad8d5c45c",
   "metadata": {},
   "source": [
    "$$TFIDF_i=TF_i*log \\frac{N}{DF_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56a1af-97b1-4cc2-b163-bcc1b9acb1e2",
   "metadata": {},
   "source": [
    "### 사이킷런의 Count 및 TF-IDF 벡터화 구현 : CountVectorizer, TfdifVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab10654-8dc4-4e74-9d98-4dd0b322bc92",
   "metadata": {},
   "source": [
    "+ 사전 데이터 가공\n",
    "+ 토큰화\n",
    "+ 텍스트 정규화\n",
    "+ 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9dd4f-a0b0-4c59-b078-9cfa25be2cd7",
   "metadata": {},
   "source": [
    "### BOW 벡터화를 위한 희소 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fafb7-a27c-4dda-a68e-cd481a3cb507",
   "metadata": {},
   "source": [
    "희소 행렬을 물리적으로 적은 메모리 공간을 차지할 수 있도록 변환해야 함\n",
    "+ COO\n",
    "+ CSR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e195018-60b8-49f3-836c-9db4fccce15b",
   "metadata": {},
   "source": [
    "#### 희소 행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8f8b9-6f04-467f-bc15-9451759c41d3",
   "metadata": {},
   "source": [
    "0이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1686fb78-3f9f-4cbe-951d-48833da0b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array( [ [ 3, 0, 1 ], [0, 2, 0 ] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a4e106-76fc-41f0-8b16-5546ea714f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fba9947-6f33-469c-bb77-335b79fd7b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e126b-7814-42fc-b522-c114901921c1",
   "metadata": {},
   "source": [
    "#### 희소 행렬 - CSR 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1b7a6-0666-46e4-94da-beb7b20ad4dc",
   "metadata": {},
   "source": [
    "COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e23f854e-87ec-4eb7-8866-c6e8f4e16a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5a95c-4575-4ee6-8f38-f167cff0efbb",
   "metadata": {},
   "source": [
    "## 텍스트 분류 실습 - 20 뉴스그룹 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed4775-1f22-456c-a246-3eed97c70f5e",
   "metadata": {},
   "source": [
    "텍스트를 피처 벡터화로 변환한 희소 행렬에 분류를 효과적으로 처리할 수 있는 알고리즘으로 로지스틱 회귀, 선형 서포트 벡터 머신, 나이브 베이즈 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e1c87-177a-4d72-bd5d-6a646d376f01",
   "metadata": {},
   "source": [
    "+ 카운트 기반과 TF-IDF 기반의 벡터화 비교\n",
    "+ 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10004ee-20ae-425d-8b50-ad628d3e1aa1",
   "metadata": {},
   "source": [
    "### 텍스트 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66fe8edf-16c0-4862-a099-9a032910b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all',random_state=156)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d29503-05c5-42e5-ba39-85e708c8a4a0",
   "metadata": {},
   "source": [
    "key값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78814d4d-672c-4805-b6ff-fe5bd2a248ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517458a-16ad-45f7-90b6-93884b216eb8",
   "metadata": {},
   "source": [
    "target 클래스 구성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1778d238-b85f-4acd-bc82-2da4b49f848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도 \n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "Name: count, dtype: int64\n",
      "target 클래스의 이름들 \n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도 \\n',pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('target 클래스의 이름들 \\n',news_data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6678c-ed4c-4dd1-b968-e428761ab6de",
   "metadata": {},
   "source": [
    "개별 데이터가 텍스트로 어떻게 구성되어 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "525a6e90-00ce-4739-ae54-6e3c6df8fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5f953-3454-402e-a6b4-341aad42a65a",
   "metadata": {},
   "source": [
    "이 중 내용을 제외한 제목 등의 다른 정보는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66226f6c-94fd-4b56-b923-1b16b6d809ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "학습 데이터 크기 11314 , 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset='train'으로 학습용(Train) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "print(type(X_train))\n",
    "\n",
    "# subset='test'으로 테스트(Test) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'),random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print('학습 데이터 크기 {0} , 테스트 데이터 크기 {1}'.format(len(train_news.data) , len(test_news.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e1745-99a5-4ba3-a144-a4a8a658fcf8",
   "metadata": {},
   "source": [
    "### 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e86a41-a5ab-4741-a6f1-5e6f8cf9b742",
   "metadata": {},
   "source": [
    "주의할 점\n",
    "\n",
    "테스트 데이터에서 CountVectorizer를 적용할 때는 반드시 학습 데이터를 이용해 fit()이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 transform 해야 한다는 것\n",
    "\n",
    "그래야 학습 시 설정된 CountVectorizer의 피처 개수와 테스트 데이터를 CountVectorizer로 변환할 피처 개수가 같아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4e727a1-036d-4e2c-a810-e5756da54761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. \n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e2e90-6991-4979-9501-6c0c1daa67db",
   "metadata": {},
   "source": [
    "이제 로지스틱 회귀를 적용해 뉴스그룹에 대한 분류를 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44867a7-db3a-4b1d-a60c-f6da9d62838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression 의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57b3bb-4c1b-428b-a1e1-e534c9c20026",
   "metadata": {},
   "source": [
    "이번에는 TF-IDF 기반으로 벡터화를 변경한 후 예측 모델 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6101d4e5-af04-477e-9012-a5fa412557fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression 의 예측 정확도는 0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환. \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c544519-8f44-4f8c-b36b-71d19a0277af",
   "metadata": {},
   "source": [
    "TF-IDF가 단순 카운트 기반보다 높은 예측 정확도를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b97f5-0159-404d-ac0a-6eb3a2d78c17",
   "metadata": {},
   "source": [
    "텍스트 분석에서 머신러닝 모델의 성능 향상을 위한 중요한 2가지 방법\n",
    "+ 최적의 ML 알고리즘 선택\n",
    "+ 최상의 피처 전처리 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c923f4-9f2e-47fe-98b8-69fa6e056c84",
   "metadata": {},
   "source": [
    "TF-IDF에서 다양한 파라미터를 적용해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57f19ef7-f40e-4c56-bbd1-fef9648cadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.690\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용.\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afb82b-cd7d-473b-aa32-070e9621f879",
   "metadata": {},
   "source": [
    "GridSearchCV를 이용해 로지스틱 회귀의 하이퍼 파라미터 최적화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47844be8-d200-4e44-81b6-a0ff8208714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Logistic Regression best C parameter : {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정. \n",
    "params = { 'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring='accuracy' , verbose=1 )\n",
    "grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n",
    "print('Logistic Regression best C parameter :',grid_cv_lr.best_params_ )\n",
    "\n",
    "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가. \n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9d105-c952-4dda-af9c-38f91242cdce",
   "metadata": {},
   "source": [
    "### 사이킷런 파이프라인 사용 및 GridSearchCV와의 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a083b24c-9580-4970-92d7-01860ed7a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 Logistic Regression 의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline생성\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))\n",
    "])\n",
    "\n",
    "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음. \n",
    "# pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능. \n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd657e85-dd6f-472a-b6b1-10efce5601d1",
   "metadata": {},
   "source": [
    "Pipeline + GridSearchCV를 적용할 때 유의할 점은 모두의 파라미터를 최적화할려면 너무 많은 튜닝 시간이 소모된다는 점이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
